{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales recurrentes\n",
    "\n",
    "Ya hemos visto que las capas densas tienen ciertas limitaciones, como capturar las relaciones espaciales entre los datos.\n",
    "\n",
    "Además, también tratan cada input de manera independiente, cuando en los problemas del mundo real muchas veces los datos de entrenamiento están relacionados de forma secuencial. Ejemplo de este tipo de análisis incluyen texto (no es lo mismo \"tengo un automóvil rojo\" que \"rojo un tengo automóvil\") o series de tiempo (variables que evolucionan en el tiempo y cuyo valor depende de su pasado, como la inversión en bolsa).\n",
    "\n",
    "Existe un tipo diferente de capas que forman lo que se conoce como **redes neuronales recurrentes** (RNN) enfocadas en resolver este tipo de problemas secuenciales.\n",
    "\n",
    "## Tipos de secuencia\n",
    "\n",
    "![imagenes](im45.png)\n",
    "\n",
    "## Comportamiento de una red usual vs red recurrente\n",
    "\n",
    "![imagenes](im46.png)\n",
    "\n",
    "![imagenes](im48.png)\n",
    "\n",
    "![imagenes](im47.png)\n",
    "\n",
    "Recordemos que en una capa de neuronas usual, el output es de la forma $f(Wx+b)$ donde $W$ son los pesos de la capa. Por otra parte, en la capa recurrente tenemos entonces dos conjuntos de pesos: los $W$ usuales y los que se entrenan con la información recurrente. Esto significa, matemáticamente, que el output **al tiempo $t$** viene dado por $h_t=f(Wx_t+Uh_{t-1}+b)$. donde $(x_1,x_2,...,x_T)$ son las salidas enviadas desde la capa anterior en los respectivos tiempos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Una desventaja de usar capas recurrentes es que las modificaciones de los pesos en cada propagación tienden a convertir a todos los pesos a 0. Para evitar esto, hay versiones mejoradas de capas recurrentes que resuelven este problema. Una de ellas son las long short term memory. \n",
    "\n",
    "Las capas LSTM tienen un *estado*, que es como una cinta transportadora que lleva el valor actual de una neurona recurrente a sí misma y a la siguiente capa.\n",
    "\n",
    "La célula LSTM tiene tiene la capacidad de modificar dicho estado, quitando información que ya no es relevante y añadiendo información nueva. Modifica este estado mediante estructuras llamadas *puertas*. Cada puerta se compone de una función de activación y sesgo propios, y un producto.\n",
    "\n",
    "#### Partes de una LSTM\n",
    "\n",
    "En primer lugar se aplica una capa de olvido, que se encarga de borrar la información que no considera relevante en función del nuevo input y la activación de la iteración anterior. \n",
    "\n",
    "![imagenes](im49.png)\n",
    "\n",
    "La puerta de entrada decide qué nueva información se va añadir al estado de la célula. \n",
    "\n",
    "![imagenes](im50.png)\n",
    "\n",
    "Para este momento, ya se tiene el estado actualizado:\n",
    "\n",
    "![imagenes](im51.png)\n",
    "\n",
    "Finalmente, con el estado actualizado, la puerta de salida produce la activación de la neurona.\n",
    "\n",
    "![imagenes](im52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "Una variación de las LSTM es la unidad GRU (gated recurrent unity). Es una versión simplificada de la LSTM que no tiene estado independiente de la activación y modifica directamente la activación de la capa anterior. La principal ventaja que presenta es que se entrena mas rápido con un funcionamiento similar.\n",
    "\n",
    "![image.png](im53.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
