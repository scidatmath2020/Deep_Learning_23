{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activación\n",
    "\n",
    "Las funciones de activación son las que añaden la no linealidad en las redes neuronales y permiten que dichas redes aprendan a separar datos que no son linealmente separables.\n",
    "\n",
    "Esto permite a las redes mapear cualquier input a cualquier output, y por ello se les considera aproximadores universales.\n",
    "\n",
    "El requisito para que una función pueda ser de activación es que sea derivable.\n",
    "\n",
    "Algunos ejemplos son:\n",
    "\n",
    "**Sigmoide**\n",
    "\n",
    "Esta función de activación toma cualquier rango de valores a la entrada y los mapea al rango de 0 a 1 a la salida. Dicho comportamiento se muestra en la siguiente figura:\n",
    "\n",
    "![imagenes](im17.png)\n",
    "\n",
    "En la actualidad esta función de activación tiene un uso limitado, y realmente su principal aplicación es la clasificación binaria.\n",
    "\n",
    "Lo anterior se debe al problema de saturación. Como se observa en la figura anterior, la función se satura a 1 cuando la entrada $x$ es muy alta, y a 0 cuando es muy baja. Esto hace que durante el entrenamiento usando el método del gradiente descendente, los gradientes calculados sean muy pequeños, dificultando así la convergencia del algoritmo.\n",
    "\n",
    "**Tangente hiperbólica**\n",
    "\n",
    "Esta función tiene un comportamiento muy similar a la sigmoidal, con la diferencia de que los valores de salida estarán en el rango de -1 a 1:\n",
    "\n",
    "![imagenes](im18.png)\n",
    "\n",
    "En este caso, la función también sufre del problema de saturación, pero ofrece la ventaja de tener una salida simétrica lo cual facilita el proceso de entrenamiento.\n",
    "\n",
    "A pesar de lo anterior es evidente que el problema de la saturación también limita la aplicabilidad de esta función de activación en Redes Neuronales.\n",
    "\n",
    "La función ReLU, que veremos a continuación, resuelve precisamente este inconveniente.\n",
    "\n",
    "**Unidad lineal rectificada ReLU**\n",
    "\n",
    "Su nombre viene de las siglas en Inglés de Rectified Linear Unit (o unidad lineal rectificada). El comportamiento de esta función se muestra en la siguiente figura:\n",
    "\n",
    "![imagenes](im19.png)\n",
    "\n",
    "Esta función generará una salida igual a cero cuando la entrada (z) sea negativa,  y una salida igual a la entrada cuando dicha esta última sea positiva.\n",
    "\n",
    "La función de activación ReLU se ha convertido en la más usada en los modelos Deep Learning durante los últimos años, lo cual se debe principalmente a:\n",
    "\n",
    "- La no existencia de saturación, como sí ocurre en las funciones sigmoidal y tanh. Lo anterior hace que el algoritmo del gradiente descendente converja mucho más rápidamente, facilitando así el entrenamiento.\n",
    "- Es más fácil de implementar computacionalmente en comparación con las otras dos funciones, que requieren el cálculo de funciones matemáticas más complejas como la exponencial.\n",
    "\n",
    "**Unidad lineal rectificada con fugas Leaky ReLU**\n",
    "\n",
    "![imagenes](im20.png)\n",
    "\n",
    "**Otras**\n",
    "\n",
    "![imagenes](im21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomendaciones de uso \n",
    "\n",
    "La primera recomendación es usar siempre la función ReLU en las capas ocultas de la Red Neuronal. Lo anterior hace que el entrenamiento sea más rápido y es mucho más probable que logre la convergencia del algoritmo del gradiente descendente.\n",
    "\n",
    "En segundo lugar, se recomienda el uso de las funciones sigmoidal o tanh únicamente para las capas de salida, por ejemplo en la implementación de un clasificador binario.\n",
    "\n",
    "Resumamos las ideas más importantes con relación a la función de activación:\n",
    "\n",
    "- La función de activación debe ser no lineal.\n",
    "- Las funciones sigmoidal, tanh y ReLU son las más usadas convencionalmente en modelos Deep Learning.\n",
    "- El principal inconveniente de las funciones sigmoidal y tanh es la saturación de sus valores de salida. Esto dificulta el proceso de entrenamiento al no permitir la rápida minimización de la función de error usando el método del Gradiente Descendente\n",
    "- Por lo anterior se sugiere usar las funciones sigmoidal y tanh únicamente en las capas de salida, para tareas de clasificación binaria.\n",
    "- ReLU es la función de activación más usada en la actualidad, pues no tiene problemas de saturación y es más fácil de implementar que las funciones sigmoidal y tanh.\n",
    "- Se recomienda el uso de la función ReLU en las capas ocultas de la red implementada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
