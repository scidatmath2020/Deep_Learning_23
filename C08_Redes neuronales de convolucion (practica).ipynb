{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales de Convolución - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoluciones\n",
    "\n",
    "La convolución es una operación entre matrices donde una matriz pequeña (*kernel o filtro*) se aplica de forma iterativa a los distintos trozos de una matriz más grande (una imagen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a ver de forma práctica el ejemplo de convolución visto en la teoría. Para ello creamos una imagen con dos troz verticales, uno blanco y otro negro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64 # 64x64 pixeles\n",
    "img_borde_vertical = np.zeros((img_size,img_size))\n",
    "\n",
    "img_borde_vertical[:, :int(img_size/2)] = 1\n",
    "\n",
    "plt.imshow(img_borde_vertical, cmap=\"gray\")\n",
    "plt.title(\"Imagen original\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a aplicarle un filtro de detección de bordes verticales (filtro Sobel vertical) y ver el output. Para ello usaremos la función de scipy `convolve2d` que realiza la convolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "filtro_sobel_vertical = np.array([[1, 0, -1],\n",
    "                                  [1, 0, -1],\n",
    "                                  [1, 0,-1]])\n",
    "\n",
    "output_convolucion = signal.convolve2d(img_borde_vertical, filtro_sobel_vertical, mode=\"valid\")\n",
    "plt.imshow(output_convolucion, cmap='gray')\n",
    "plt.title(\"Output de la convolución con filtro vertical\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que efectivamente, ha detectado el borde de separación entre la zona blanca y la negra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a cargar una imagen de prueba que tiene `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "img_original = misc.ascent()\n",
    "\n",
    "plt.imshow(img_original, cmap='gray')\n",
    "plt.title(\"Imagen Original\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar el mismo filtro de detección de bordes verticales a la foto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_convolucion = signal.convolve2d(img_original, filtro_sobel_vertical)\n",
    "plt.imshow(np.absolute(output_convolucion), cmap='gray')\n",
    "plt.title(\"Output de la convolución con filtro vertical\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar la misma técnica y cambiar el kernel para detectar bordes horizontales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro_sobel_horizontal = np.array([[-1,-1,-1],\n",
    "                                    [0, 0, 0],\n",
    "                                    [1, 1,1]])\n",
    "\n",
    "output_convolucion = signal.convolve2d(img_original, filtro_sobel_horizontal)\n",
    "plt.imshow(np.absolute(output_convolucion), cmap='gray')\n",
    "plt.title(\"Output de la convolución con filtro horizontal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales de Convolución - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos\n",
    "\n",
    "En este caso vamos a usar un dataset nuevo, el [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist), que es un dataset más moderno que el dataset de dígitos MNIST, que hoy en día se considera un dataset demasiado \"facil\", y que puede ser resuelto con modelos que no están relacionados con la vision artificial (como los SVMs).\n",
    "\n",
    "En concreto el dataset Fashion MNIST consiste en 60,000 fotos (28x28 pixeles como el dataset de dígitos) de prendas de ropa y su objetivo es clasificar las imágenes respecto al tipo de ropa mostrado (10 tipos distintos de artículos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](im44.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 10 clases objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podemos usar `np.bincount` para contar cuantas observaciones hay de cada clase de prenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que las clases están balanceadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos la variable objetivo a vectores one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_imagen(i):\n",
    "    plt.imshow(x_train[i], cmap=\"gray\")\n",
    "    plt.title(\"Clase de prenda: {}\".format(y_train[i]))\n",
    "    \n",
    "dibujar_imagen(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, vamos a usar una red Densa y la vamos a evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplanamos las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_plano = x_train.reshape(x_train.shape[0],28*28)\n",
    "x_test_plano = x_test.reshape(x_test.shape[0],28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_plano[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_denso = Sequential()\n",
    "modelo_denso.add(Dense(128, activation=\"relu\", input_shape=(784,)))\n",
    "modelo_denso.add(Dropout(0.2))\n",
    "modelo_denso.add(Dense(256, activation=\"relu\"))\n",
    "modelo_denso.add(Dropout(0.2))\n",
    "modelo_denso.add(Dense(128, activation=\"relu\"))\n",
    "modelo_denso.add(Dropout(0.2))\n",
    "modelo_denso.add(Dense(np.unique(y_train).shape[0], activation=\"softmax\"))\n",
    "\n",
    "modelo_denso.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", \n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "modelo_denso.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_denso.fit(x_train_plano, y_train_one_hot, epochs=50, batch_size=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_denso.evaluate(x_test_plano, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a usar una red de convolución (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "modelo_cnn = Sequential()\n",
    "modelo_cnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "modelo_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_cnn.add(Dropout(0.25))\n",
    "modelo_cnn.add(Flatten())\n",
    "modelo_cnn.add(Dense(32, activation='relu'))\n",
    "modelo_cnn.add(Dropout(0.5))\n",
    "modelo_cnn.add(Dense(num_classes, activation='softmax'))\n",
    "modelo_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 20s 328us/step - loss: 4.5171 - accuracy: 0.1835\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 19s 324us/step - loss: 1.9990 - accuracy: 0.2054\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.9596 - accuracy: 0.2201\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.9314 - accuracy: 0.2289\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 1.9095 - accuracy: 0.2376\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 20s 328us/step - loss: 1.9008 - accuracy: 0.2396\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 1.8852 - accuracy: 0.2492\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 19s 324us/step - loss: 1.8482 - accuracy: 0.2663\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.8327 - accuracy: 0.2790\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.8215 - accuracy: 0.2872\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 20s 328us/step - loss: 1.8038 - accuracy: 0.2966\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.7790 - accuracy: 0.3013\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.7432 - accuracy: 0.3126\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 20s 327us/step - loss: 1.7132 - accuracy: 0.3246\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 20s 327us/step - loss: 1.6975 - accuracy: 0.3317\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 20s 325us/step - loss: 1.6917 - accuracy: 0.3335\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 20s 329us/step - loss: 1.6731 - accuracy: 0.3442\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.6576 - accuracy: 0.3520\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 20s 334us/step - loss: 1.6065 - accuracy: 0.3692\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.5577 - accuracy: 0.3900\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 1.5367 - accuracy: 0.3909\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 22s 359us/step - loss: 1.5090 - accuracy: 0.3994\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 20s 325us/step - loss: 1.5046 - accuracy: 0.3974\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 1.4980 - accuracy: 0.3975\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 1.4888 - accuracy: 0.4006\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 1.4768 - accuracy: 0.4047\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 1.4761 - accuracy: 0.4022\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 19s 322us/step - loss: 1.4683 - accuracy: 0.4086\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 1.4609 - accuracy: 0.4091\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 22s 363us/step - loss: 1.4523 - accuracy: 0.4148\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 1.4466 - accuracy: 0.4176\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 1.4457 - accuracy: 0.4183\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 1.4304 - accuracy: 0.4250\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 1.4292 - accuracy: 0.4251\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 20s 338us/step - loss: 1.4316 - accuracy: 0.4240\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 1.4252 - accuracy: 0.4264\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 1.4230 - accuracy: 0.4282\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 22s 360us/step - loss: 1.4207 - accuracy: 0.4325\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 21s 345us/step - loss: 1.4125 - accuracy: 0.4338\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 1.4123 - accuracy: 0.4361\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 19s 322us/step - loss: 1.4063 - accuracy: 0.4381\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 1.4058 - accuracy: 0.4398\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 19s 321us/step - loss: 1.4048 - accuracy: 0.4400\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 19s 321us/step - loss: 1.4032 - accuracy: 0.4433\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 19s 324us/step - loss: 1.4017 - accuracy: 0.4429\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 21s 354us/step - loss: 1.3931 - accuracy: 0.4428\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 23s 389us/step - loss: 1.3741 - accuracy: 0.4517\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 1.3676 - accuracy: 0.4560\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 22s 361us/step - loss: 1.3404 - accuracy: 0.4706\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 1.3234 - accuracy: 0.4744\n"
     ]
    }
   ],
   "source": [
    "modelo_cnn.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "modelo_cnn.fit(x_train, y_train_one_hot, epochs=50, batch_size=1000, verbose=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cnn.evaluate(x_test, y_test_one_hot, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Generalmente se concatenan múltiples capas convolucionales en una red de convolución CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "filter_pixel=3\n",
    "noise = 1\n",
    "droprate=0.25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), padding=\"same\",\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation='relu',padding=\"same\"))#1\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation='relu',padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,use_bias=False, activation=\"relu\")) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))      \n",
    "\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 587s 10ms/step - loss: 0.4457 - accuracy: 0.8435 - val_loss: 0.3503 - val_accuracy: 0.8749\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test_one_hot), \n",
    "          shuffle=True,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test_one_hot, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
