{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrones multicapa (multilier perceptrons)\n",
    "\n",
    "Estudiaremos la estructura más elemental de la red neuronal: los perceptrones multicapa. \n",
    "\n",
    "Supongamos que tenemos los datos de entrada $x_1,x_2,...,x_n$.\n",
    "\n",
    "![imagenes](im08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso más simple de una clasificación binaria, un perceptrón tendrá la siguiente función de activación: \n",
    "\n",
    "$$f(x)=\\left\\{\\begin{array}{cl}1&\\mbox{si }w_1x_1+...+w_nx_n+b>0\\\\0&\\mbox{en otro caso}\\end{array}\\right.$$\n",
    "\n",
    "donde $x_1,...,x_n$ son los valores de entrada, $w_1,...,w_n$ son los pesos y $b$ es el sesgo.\n",
    "\n",
    "Notemos que esta es muy parecido a la manera en que funcionan las [máquinas de soporte vectorial](https://github.com/scidatmath2020/Machine-Learning/blob/main/C12.01%20M%C3%A1quinas%20de%20soporte%20vectoria%20(teor%C3%ADa).ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje del perceptrón\n",
    "\n",
    "Como hemos visto, los únicos parámetros que tiene el perceptrón son los pesos de la sinapsis. El objetivo es encontrar aquellos pesos óptimos en el siguiente sentido:\n",
    "\n",
    "Supongamos una tabla con $S$ observaciones (renglones), $D=\\{(\\boldsymbol{x_1},d_1),...,(\\boldsymbol{x_S},d_S)\\}$, donde $\\boldsymbol{x_i}=(x_{i1},x_{i2},...,x_{in})$ son las $n$ columnas explicativas, y $d_j$ es la columna que queremos explicar (1 o 0).\n",
    "\n",
    "Sea $x_{j0}=1$ para todo renglón $j$ (con esta condición no necesitamos $b$ en la función de activación). \n",
    "\n",
    "Si $w_i(t)$ es el peso $i$ de la sinapsis en el paso $t$, entonces el error máximo que queremos cometer es $$\\gamma = \\frac{1}{s}\\sum_{j=1}^s|d_j-y_j(t)|$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera, el algoritmo trabaja así:\n",
    "\n",
    "1. Inicializamos el proceso de forma aleatoria; por ejemplo, tomando cada peso igual a 0 o a un número aleatorio.\n",
    "\n",
    "2. Para cada renglón $j$ de la tabla $D$: \n",
    "\n",
    "    a) Calculamos la salida de la función de activación:\n",
    "    $$y_j(t)=f(w(t)\\bullet x_j)=f(w_0(t)x_{j0}+w_1(t)x_{j1}(t)+...+w_n(t)x_{jn}(t))$$\n",
    "    \n",
    "    b) Actualizamos los pesos sinápticos, para cada variable $i$ de la observación $j$:\n",
    "    $$w_i(t+1)=w_i(t)+(d_j-y_j(t))x_{ji}$$\n",
    "    \n",
    "3. Evaluamos si hemos llegado a cumplir un criterio de convergencia. Si el error total es menor que el deseado, paramos. En caso contrario continuamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos ahora que el algoritmo de Máquina de soporte vectorial podía crear fronteras de decisión para separar datos no linealmente separables, ya que hacía una transformación de los datos para llevarlos a una dimensión más alta. Vamos a verlo brevemente [aquí](https://www.youtube.com/watch?v=OdlNM96sHio&ab_channel=udiprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptrón clásico no puede crear fronteras de decisión no lineales. Por ello, en las redes neuronales existen una variedad de funciones de activación más avanzadas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, el perceptrón multicapa es una red neuronal formada por varias perceptrones: \n",
    "![imagenes](im09.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
