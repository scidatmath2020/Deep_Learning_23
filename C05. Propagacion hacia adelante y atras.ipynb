{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagación hacia adelante\n",
    "\n",
    "Recordemos el funcionamiento de un perceptrón multicapa:\n",
    "\n",
    "Tenemos los datos $\\boldsymbol{x_1}=(x_{11},x_{12},x_{13},...,x_{1n},y_1)$, $\\boldsymbol{x_2}=(x_{21},x_{22},x_{23},...,x_{2n},y_2)$,...,$\\boldsymbol{x_m}=(x_{m1},x_{m2},x_{m3},...,x_{mn},y_m)$\n",
    "\n",
    "![imagenes](im12.png)\n",
    "\n",
    "Como únicamente existe una variable objetivo (la $Y$), solo hay una neurona en la capa de salida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de aprendizaje de la red se puede esquematizar así: \n",
    "![imagenes](im13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **propagación hacia adelante** se refiera al proceso con el cual la red realiza las predicciones. \n",
    "\n",
    "**Observaciónes** \n",
    "* $w_{ij}$ es el peso que va la de neurona $i$ a la $j$ entre capas;\n",
    "* $n_{ij}$ es la neurona $j$ de la capa $i$\n",
    "![imagenes](im14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo funciona?**\n",
    "\n",
    "Primero, inicializamos cuando recibimos la información de un solo renglón en la capa 0: ![imagen](im15.png)![imagen](im16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de avanzar, haremos un cambio en la forma en que incluimos al sesgo. Ahora lo incluiremos dentro de la matriz:\n",
    "![imagenes](im23.png)\n",
    "\n",
    "# Propagación hacia atrás.\n",
    "\n",
    "Consiste en, tomando el error de predicción como valor inicial, ir recorriendo la red neuronal hacia atrás modificando los pesos en cada capa.\n",
    "\n",
    "![imagenes](im24.png)\n",
    "\n",
    "## Paso 1.\n",
    "\n",
    "Consiste en usar la función de pérdida escogida para calcular el error de predicción de la observación.\n",
    "\n",
    "Para problemas de regresión, usualmente se usa el **error cuadrático**: \n",
    "\n",
    "$$J(\\boldsymbol{w},\\boldsymbol{x_i},y_i)=\\frac{1}{2}(h_{\\boldsymbol{w}}(\\boldsymbol{x_i})-y_i^2)^2$$\n",
    "\n",
    "Para problemas de clasificación binaria, usualmente se usa la **pérdida logarítimica**: \n",
    "\n",
    "$$J(\\boldsymbol{w},\\boldsymbol{x_i},y_i)=-y_i\\log(h_{\\boldsymbol{w}}(\\boldsymbol{x_i}))+(1-y_i)\\log(1-h_{\\boldsymbol{w}}(\\boldsymbol{x_i}))$$\n",
    "\n",
    "En cualquier caso, denotamos por $\\delta_1^{(2)}$ al error (célula 1 de la capa 2)\n",
    "\n",
    "## Paso 2.\n",
    "\n",
    "Propagamos el error hacia la capa oculta.\n",
    "![imagenes](im25.png)\n",
    "\n",
    "## Paso 3.\n",
    "\n",
    "Propagamos el error hacia la capa de entrada.\n",
    "![imagenes](im26.png)\n",
    "\n",
    "## Paso 4.\n",
    "\n",
    "Ahora actualizamos los pesos. Para cada peso de cada neurona hacemos $$\\boldsymbol{w}_i=\\boldsymbol{w_i}-\\alpha\\delta_i\\mbox{input}_i$$ donde $\\alpha$ es el **radio de aprendizaje** e **input** es la información entrante a dicha neurona."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
